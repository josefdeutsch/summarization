
Good. Here are clean, one-sentence examples for each — aligned with your intuition:

1️⃣ **Deterministic constraints belong in deterministic graders.**
→ *“Is the page range strictly inside p120–150?”*

2️⃣ **Semantic judgments belong in LLM graders.**
→ *“Is this takeaway conceptually deep and non-trivial?”*

3️⃣ **Relative improvement belongs in comparison graders.**
→ *“Is version A better than version B, or are both worse than baseline?”*

Even shorter mental shortcut:

1. **Is it correct?**
2. **How good is it?**
3. **Which is better?**

Memorize that triad.


| Type              | Deterministic? | Needs LLM? | Use Case                            | Output Nature     |
| ----------------- | -------------- | ---------- | ----------------------------------- | ----------------- |
| Simple Grader     | Yes            | No         | Format, numeric, structural rules   | Binary pass/fail  |
| Scoring Grader    | No             | Yes        | Quality, centrality, non-triviality | Scalar score      |
| Comparison Grader | No             | Often Yes  | A vs B, reference match             | Relative judgment |

| Dimension                        | Simple Grader (Deterministic)                             | Scoring Grader (LLM Judge)                 | Comparison Grader                        |
| -------------------------------- | --------------------------------------------------------- | ------------------------------------------ | ---------------------------------------- |
| **Core Purpose**                 | Enforce hard rules                                        | Evaluate quality                           | Compare outputs                          |
| **What It Asks**                 | “Did it obey the rule?”                                   | “How good is this?”                        | “Is A better than B?”                    |
| **Determinism**                  | Fully deterministic                                       | Probabilistic                              | Usually probabilistic                    |
| **Uses LLM?**                    | No                                                        | Yes                                        | Often yes                                |
| **Typical Implementation**       | Python / regex / schema                                   | LLM rubric scoring                         | LLM preference or similarity             |
| **Failure Type**                 | Binary violation                                          | Weak or strong quality                     | Relative superiority                     |
| **Output Form**                  | Pass / Fail (0 or 1)                                      | Score (0–1 or 1–5)                         | A wins / B wins / tie                    |
| **Regression Stability**         | Very high                                                 | Medium                                     | Medium                                   |
| **Use In Regression Domain**     | Heavy use                                                 | Rare                                       | Rare                                     |
| **Use In Generalization Domain** | Light use                                                 | Heavy use                                  | Optional                                 |
| **Good For**                     | JSON validity, numeric bounds, format checks, page ranges | Non-triviality, centrality, clarity, depth | Prompt A vs Prompt B, version comparison |
| **Bad For**                      | Evaluating insight depth                                  | Enforcing numeric precision                | Absolute correctness                     |
| **Maintenance Cost**             | Low                                                       | Medium                                     | Medium–High                              |
| **Risk Profile**                 | False negatives only if logic wrong                       | Score variance                             | Judgment variance                        |
